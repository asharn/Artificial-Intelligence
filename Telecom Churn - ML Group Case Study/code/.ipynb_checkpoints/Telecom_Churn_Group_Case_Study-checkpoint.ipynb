{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Required Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing helpful pacakages to load in\n",
    "\n",
    "import numpy as np   # linear algebra\n",
    "import pandas as pd   # data processing, CSV file I/O\n",
    "\n",
    "#For plotting\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 300)\n",
    "pd.set_option(\"display.max_rows\", 300)\n",
    "\n",
    "# Until fuction: line seperator\n",
    "def print_dashes_and_ln():\n",
    "    print('-'*100, '\\n')\n",
    "    \n",
    "# Formatter to display all float format in 2 decimal format\n",
    "pd.options.display.float_format = '{:.2f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data from csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the loan data\n",
    "\n",
    "rawchurn = pd.read_csv('telecom_churn_data.csv' , sep = ',',engine = 'python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snapshot of Chrun Data\n",
    "print(rawchurn.head(10)); print_dashes_and_ln();\n",
    "print(rawchurn.shape); print_dashes_and_ln();\n",
    "print('This dataset has ' + str(rawchurn.shape[0]) + ' rows, and ' + str(rawchurn.shape[1]) + ' columns'); print_dashes_and_ln();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawchurn.info(verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawchurn.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to create id columns, date columns, category columns and numeric columns to understand the data better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_columns = ['mobile_number', 'circle_id']\n",
    "\n",
    "date_columns = [\n",
    "             'date_of_last_rech_data_6',\n",
    "             'date_of_last_rech_data_7',\n",
    "             'date_of_last_rech_data_8',\n",
    "             'date_of_last_rech_data_9',\n",
    "             'date_of_last_rech_6',\n",
    "             'date_of_last_rech_7',\n",
    "             'date_of_last_rech_8',\n",
    "             'date_of_last_rech_9',\n",
    "             'last_date_of_month_6',\n",
    "             'last_date_of_month_7',\n",
    "             'last_date_of_month_8',\n",
    "             'last_date_of_month_9'  \n",
    "              ]\n",
    "\n",
    "category_columns = [\n",
    "             'fb_user_6',\n",
    "             'fb_user_7',\n",
    "             'fb_user_8',\n",
    "             'fb_user_9',\n",
    "             'night_pck_user_6',\n",
    "             'night_pck_user_7',\n",
    "             'night_pck_user_8',\n",
    "             'night_pck_user_9'             \n",
    "                  ]\n",
    "\n",
    "numeric_columns = [column for column in rawchurn.columns if column not in id_columns + date_columns + category_columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the number of columns in each list\n",
    "print((\"id_Columns:{}\").format(len(id_columns))); print_dashes_and_ln();\n",
    "print((\"date_columns:{}\").format(len(date_columns))); print_dashes_and_ln();\n",
    "print((\"category_columns:{}\").format(len(category_columns))); print_dashes_and_ln();\n",
    "print((\"numeric_columns:{}\").format(len(numeric_columns))); print_dashes_and_ln();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((\"Total no. of Columns:{}\").format(len(id_columns) +len(date_columns) + len(category_columns) + len(numeric_columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have taken care of the classification of all the variables that are present in churn data sets. Now we will check for missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is 99999 rows and 226 columns. There are many columns which Contain NaN. Lets identify those columns and get rid of them\n",
    "rawchurn.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will first check all the recharge columns\n",
    "recharge_columns = [\n",
    "                  'av_rech_amt_data_6', 'av_rech_amt_data_7', 'av_rech_amt_data_8', 'av_rech_amt_data_9',\n",
    "                  'max_rech_data_6', 'max_rech_data_7', 'max_rech_data_8', 'max_rech_data_9',\n",
    "                  'total_rech_data_6', 'total_rech_data_7', 'total_rech_data_8', 'total_rech_data_9'\n",
    "                 ]\n",
    "\n",
    "rawchurn[recharge_columns].describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can see that minimun value of mostly columns is zero and some cases it is one so we can replace the missing value with zero. It means that users has not recharged at all.\n",
    "\n",
    "rawchurn[recharge_columns] = rawchurn[recharge_columns].apply(lambda x: x.fillna(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking there is no missing values present\n",
    "rawchurn[recharge_columns].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will drop date and id columns\n",
    "print(\"Shape before dropping: \", rawchurn.shape); print_dashes_and_ln();\n",
    "rawchurn = rawchurn.drop(id_columns + date_columns, axis=1)\n",
    "print(\"Shape after dropping: \", rawchurn.shape); print_dashes_and_ln();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For categorical columns, we will replace missing values with some thing and we will call this as new category. Lets choose -1.\n",
    "rawchurn[category_columns] = rawchurn[category_columns].apply(lambda x: x.fillna(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking there is no missing values present\n",
    "rawchurn[category_columns].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will now drop variables which has more than 70% missing data\n",
    "\n",
    "initial_columns = rawchurn.shape[1]\n",
    "\n",
    "THRESHOLD = 0.7\n",
    "\n",
    "include_columns = list(rawchurn.apply(lambda column: True if column.isnull().sum()/rawchurn.shape[0] < THRESHOLD else False))\n",
    "\n",
    "drop_missing = pd.DataFrame({'features':rawchurn.columns , 'include': include_columns})\n",
    "drop_missing.loc[drop_missing.include == True,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the columns now\n",
    "rawchurn = rawchurn.loc[:, include_columns]\n",
    "\n",
    "dropped_cols = rawchurn.shape[1] - initial_columns\n",
    "print(\"columns dropped.{0}\".format(dropped_cols)); print_dashes_and_ln();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawchurn.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling all NA's with Zero for Now\n",
    "rawchurn.fillna(0, inplace=True)\n",
    "sum(rawchurn.isnull().sum()>0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Return Monthwise Columns List. Returns arrays of columns belonging to 6,7,8,9 month separately.\n",
    "# Also returns an array of columns that are not month specific as common columns.\n",
    "\n",
    "def returnColumnsByMonth(df):\n",
    "    column_Month_6 = []\n",
    "    column_Month_7 = []\n",
    "    column_Month_8 = []\n",
    "    column_Month_9 = []\n",
    "    column_Common = []\n",
    "    for eachColumns in df.columns:\n",
    "        if((eachColumns.find(\"_6\") >=0) | (eachColumns.find(\"jun_\") >=0)):\n",
    "            column_Month_6.append(eachColumns)\n",
    "        elif((eachColumns.find(\"_7\") >=0) | (eachColumns.find(\"jul_\") >=0)):\n",
    "            column_Month_7.append(eachColumns)\n",
    "        elif((eachColumns.find(\"_8\") >= 0) | (eachColumns.find(\"aug_\") >=0)):\n",
    "            column_Month_8.append(eachColumns)\n",
    "        elif((eachColumns.find(\"_9\") >=0) | (eachColumns.find(\"sep_\") >=0)):\n",
    "            column_Month_9.append(eachColumns)\n",
    "        else:\n",
    "            column_Common.append(eachColumns)\n",
    "    return column_Month_6, column_Month_7, column_Month_8, column_Month_9, column_Common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Columns Monthwise & Basic Understanding of Columns\n",
    "column_Month_6, column_Month_7, column_Month_8, column_Month_9, column_Common = returnColumnsByMonth(rawchurn)\n",
    "\n",
    "print(\"Month 6 Columns Count ==> {}\".format(len(column_Month_6)))\n",
    "print(\"Month 7 Columns Count ==> {}\".format(len(column_Month_7)))\n",
    "print(\"Month 8 Columns Count ==> {}\".format(len(column_Month_8)))\n",
    "print(\"Month 9 Columns Count ==> {}\".format(len(column_Month_9)))\n",
    "print(\"Common Columns Count ==> {}\".format(len(column_Common)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Months are having same type of columns So lets see the columns in general\n",
    "print (\"\\nMonth based Columns:\\n \\t\\t==> {}\".format(np.array(column_Month_6)))\n",
    "print (\"\\nCommon Columns:\\n \\t\\t==> {}\".format(np.array(column_Common)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derive Columns Total_Recharge_Amount from 6th and 7th Month total_rech_amt\n",
    "rawchurn['Total_Recharge_Amount'] = rawchurn['total_rech_amt_6'] + rawchurn['total_rech_amt_7']\n",
    "\n",
    "# As per Upgrad guideline, we have to look at 70th percentile of the average recharge amount in the first two months (the good phase)\n",
    "print(rawchurn['Total_Recharge_Amount'].describe(percentiles = [0.7])); print_dashes_and_ln();\n",
    "print(\"\\n70% of Total Recharge Amount of first 2 months are {}\".format(rawchurn['Total_Recharge_Amount'].describe(percentiles = [0.7])[5])); print_dashes_and_ln();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter High Value Customer with more than or equal to 70th percentile amount\n",
    "rawchurn = rawchurn[rawchurn['Total_Recharge_Amount'] > 737].reset_index(drop=True)\n",
    "print(\"\\nTotal High Value Customer Count ==> {}\".format(rawchurn.shape)); print_dashes_and_ln();\n",
    "rawchurn.drop(columns=['Total_Recharge_Amount'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tag churners and remove attributes of the churn phase\n",
    "# calculate total incoming and outgoing minutes of usage\n",
    "rawchurn['total_calls_mou_9'] = rawchurn.total_ic_mou_9 + rawchurn.total_og_mou_9\n",
    "rawchurn['total_internet_usage_9'] =  rawchurn.vol_2g_mb_9 + rawchurn.vol_3g_mb_9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag the churned customers (churn=1, else 0)\n",
    "rawchurn['churn'] = rawchurn.apply(lambda row: 1 if (row.total_calls_mou_9 == 0 and row.total_internet_usage_9 == 0) else 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete derived variables\n",
    "rawchurn = rawchurn.drop(['total_calls_mou_9', 'total_internet_usage_9'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change data type to category\n",
    "rawchurn.churn = rawchurn.churn.astype(\"category\")\n",
    "\n",
    "# print churn ratio\n",
    "print(\"Churn Ratio:\"); print_dashes_and_ln();\n",
    "print(rawchurn.churn.value_counts()*100/rawchurn.shape[0]); print_dashes_and_ln();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Churn is 8.64% which indicates an unbalanced datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns with '9'\n",
    "rawchurn = rawchurn.filter(regex='[^9]$', axis=1)\n",
    "rawchurn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all names that end with 9\n",
    "col_9_names = rawchurn.filter(regex='9$', axis=1).columns\n",
    "\n",
    "# update numeric_columns and category_columns list\n",
    "category_columns = [col for col in category_columns if col not in col_9_names]\n",
    "category_columns.append('churn')\n",
    "numeric_columns = [col for col in rawchurn.columns if col not in category_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets look also at difference between the 8th month and the previous months\n",
    "\n",
    "rawchurn['arpu_diff'] = rawchurn.arpu_8 - ((rawchurn.arpu_6 + rawchurn.arpu_7)/2)\n",
    "\n",
    "rawchurn['onnet_mou_diff'] = rawchurn.onnet_mou_8 - ((rawchurn.onnet_mou_6 + rawchurn.onnet_mou_7)/2)\n",
    "\n",
    "rawchurn['offnet_mou_diff'] = rawchurn.offnet_mou_8 - ((rawchurn.offnet_mou_6 + rawchurn.offnet_mou_7)/2)\n",
    "\n",
    "rawchurn['roam_ic_mou_diff'] = rawchurn.roam_ic_mou_8 - ((rawchurn.roam_ic_mou_6 + rawchurn.roam_ic_mou_7)/2)\n",
    "\n",
    "rawchurn['roam_og_mou_diff'] = rawchurn.roam_og_mou_8 - ((rawchurn.roam_og_mou_6 + rawchurn.roam_og_mou_7)/2)\n",
    "\n",
    "rawchurn['loc_og_mou_diff'] = rawchurn.loc_og_mou_8 - ((rawchurn.loc_og_mou_6 + rawchurn.loc_og_mou_7)/2)\n",
    "\n",
    "rawchurn['std_og_mou_diff'] = rawchurn.std_og_mou_8 - ((rawchurn.std_og_mou_6 + rawchurn.std_og_mou_7)/2)\n",
    "\n",
    "rawchurn['isd_og_mou_diff'] = rawchurn.isd_og_mou_8 - ((rawchurn.isd_og_mou_6 + rawchurn.isd_og_mou_7)/2)\n",
    "\n",
    "rawchurn['spl_og_mou_diff'] = rawchurn.spl_og_mou_8 - ((rawchurn.spl_og_mou_6 + rawchurn.spl_og_mou_7)/2)\n",
    "\n",
    "rawchurn['total_og_mou_diff'] = rawchurn.total_og_mou_8 - ((rawchurn.total_og_mou_6 + rawchurn.total_og_mou_7)/2)\n",
    "\n",
    "rawchurn['loc_ic_mou_diff'] = rawchurn.loc_ic_mou_8 - ((rawchurn.loc_ic_mou_6 + rawchurn.loc_ic_mou_7)/2)\n",
    "\n",
    "rawchurn['std_ic_mou_diff'] = rawchurn.std_ic_mou_8 - ((rawchurn.std_ic_mou_6 + rawchurn.std_ic_mou_7)/2)\n",
    "\n",
    "rawchurn['isd_ic_mou_diff'] = rawchurn.isd_ic_mou_8 - ((rawchurn.isd_ic_mou_6 + rawchurn.isd_ic_mou_7)/2)\n",
    "\n",
    "rawchurn['spl_ic_mou_diff'] = rawchurn.spl_ic_mou_8 - ((rawchurn.spl_ic_mou_6 + rawchurn.spl_ic_mou_7)/2)\n",
    "\n",
    "rawchurn['total_ic_mou_diff'] = rawchurn.total_ic_mou_8 - ((rawchurn.total_ic_mou_6 + rawchurn.total_ic_mou_7)/2)\n",
    "\n",
    "rawchurn['total_rech_num_diff'] = rawchurn.total_rech_num_8 - ((rawchurn.total_rech_num_6 + rawchurn.total_rech_num_7)/2)\n",
    "\n",
    "rawchurn['total_rech_amt_diff'] = rawchurn.total_rech_amt_8 - ((rawchurn.total_rech_amt_6 + rawchurn.total_rech_amt_7)/2)\n",
    "\n",
    "rawchurn['max_rech_amt_diff'] = rawchurn.max_rech_amt_8 - ((rawchurn.max_rech_amt_6 + rawchurn.max_rech_amt_7)/2)\n",
    "\n",
    "rawchurn['total_rech_data_diff'] = rawchurn.total_rech_data_8 - ((rawchurn.total_rech_data_6 + rawchurn.total_rech_data_7)/2)\n",
    "\n",
    "rawchurn['max_rech_data_diff'] = rawchurn.max_rech_data_8 - ((rawchurn.max_rech_data_6 + rawchurn.max_rech_data_7)/2)\n",
    "\n",
    "rawchurn['av_rech_amt_data_diff'] = rawchurn.av_rech_amt_data_8 - ((rawchurn.av_rech_amt_data_6 + rawchurn.av_rech_amt_data_7)/2)\n",
    "\n",
    "rawchurn['vol_2g_mb_diff'] = rawchurn.vol_2g_mb_8 - ((rawchurn.vol_2g_mb_6 + rawchurn.vol_2g_mb_7)/2)\n",
    "\n",
    "rawchurn['vol_3g_mb_diff'] = rawchurn.vol_3g_mb_8 - ((rawchurn.vol_3g_mb_6 + rawchurn.vol_3g_mb_7)/2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualise Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change columns types\n",
    "category_columns1 = [\n",
    "             'fb_user_6',\n",
    "             'fb_user_7',\n",
    "             'fb_user_8',\n",
    "             'night_pck_user_6',\n",
    "             'night_pck_user_7',\n",
    "             'night_pck_user_8',             \n",
    "                  ]\n",
    "rawchurn[numeric_columns] = rawchurn[numeric_columns].apply(pd.to_numeric)\n",
    "rawchurn[category_columns1] = rawchurn[category_columns1].apply(lambda column: column.astype(\"category\"), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create plotting functions\n",
    "def data_type(variable):\n",
    "    if variable.dtype == np.int64 or variable.dtype == np.float64:\n",
    "        return 'numerical'\n",
    "    elif variable.dtype == 'category':\n",
    "        return 'categorical'\n",
    "    \n",
    "def univariate(variable, stats=True):\n",
    "    \n",
    "    if data_type(variable) == 'numerical':\n",
    "        sns.distplot(variable)\n",
    "        if stats == True:\n",
    "            print(variable.describe())\n",
    "    \n",
    "    elif data_type(variable) == 'categorical':\n",
    "        sns.countplot(variable)\n",
    "        if stats == True:\n",
    "            print(variable.value_counts())\n",
    "            \n",
    "    else:\n",
    "        print(\"Invalid variable passed: either pass a numeric variable or a categorical vairable.\")\n",
    "        \n",
    "def bivariate(var1, var2):\n",
    "    if data_type(var1) == 'numerical' and data_type(var2) == 'numerical':\n",
    "        sns.regplot(var1, var2)\n",
    "    elif (data_type(var1) == 'categorical' and data_type(var2) == 'numerical') or (data_type(var1) == 'numerical' and data_type(var2) == 'categorical'):        \n",
    "        sns.boxplot(var1, var2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA For all the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "univariate(rawchurn.arpu_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "univariate(rawchurn.loc_og_t2o_mou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "univariate(rawchurn.std_og_t2o_mou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "univariate(rawchurn.onnet_mou_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bivariate EDA\n",
    "bivariate(rawchurn.churn, rawchurn.aon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bivariate(rawchurn.sep_vbc_3g, rawchurn.churn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bivariate(rawchurn.spl_og_mou_8, rawchurn.churn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(rawchurn.churn, rawchurn.night_pck_user_8, normalize='columns')*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(rawchurn.churn, rawchurn.sachet_3g_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = rawchurn.groupby('churn')['aon'].agg(['mean']).reset_index()\n",
    "p = sns.barplot(x='churn', y='mean', data=X1)\n",
    "p.set_xticklabels(['Not-Churn', 'Churn'],rotation=30)\n",
    "p.set_ylabel('Average Age in Network')\n",
    "plt.title('Average Age in Network between Churn and Not-Churn subscriber')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Churn subscriber is having less average AON than Non-Churn Subscriber. Hence subsribers which has high AON has less chances of Churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capping the outliers with k-sigma technique\n",
    "def cap_outliers(array, k=3):\n",
    "    upper_limit = array.mean() + k*array.std()\n",
    "    lower_limit = array.mean() - k*array.std()\n",
    "    array[array<lower_limit] = lower_limit\n",
    "    array[array>upper_limit] = upper_limit\n",
    "    return array\n",
    "rawchurn[numeric_columns] = rawchurn[numeric_columns].apply(cap_outliers, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize = (40,20))      \n",
    "sns.heatmap(rawchurn.corr(),annot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Due to a large number of variables, we cannot visualize the correlation matrix properly. We will address this after PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# # Data standardization and preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing important libraries\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting Churn datatype into numeric\n",
    "rawchurn['churn'] = pd.to_numeric(rawchurn['churn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide data into train and test\n",
    "X = rawchurn.drop(\"churn\", axis = 1)\n",
    "y = rawchurn.churn\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 100, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print no.of features and shapes of train and test sets\n",
    "print(\"Number of Features ==> {}\".format(len(X.columns))); print_dashes_and_ln();\n",
    "X_train.shape\n",
    "y_train.shape\n",
    "X_test.shape\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# aggregate the categorical variables\n",
    "train.groupby('night_pck_user_6').churn.mean()\n",
    "train.groupby('night_pck_user_7').churn.mean()\n",
    "train.groupby('night_pck_user_8').churn.mean()\n",
    "train.groupby('fb_user_6').churn.mean()\n",
    "train.groupby('fb_user_7').churn.mean()\n",
    "train.groupby('fb_user_8').churn.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace categories with aggregated values in each categorical column\n",
    "mapping = {'night_pck_user_6' : {-1: 0.099087, 0: 0.064849, 1: 0.095833},\n",
    "           'night_pck_user_7' : {-1: 0.106837, 0: 0.053456, 1: 0.075117},\n",
    "           'night_pck_user_8' : {-1: 0.123411, 0: 0.028631, 1: 0.033981},\n",
    "           'fb_user_6'        : {-1: 0.099087, 0: 0.081703, 1: 0.063913},\n",
    "           'fb_user_7'        : {-1: 0.106837, 0: 0.070084, 1: 0.052000},\n",
    "           'fb_user_8'        : {-1: 0.123411, 0: 0.062718, 1: 0.022138}\n",
    "          }\n",
    "X_train.replace(mapping, inplace = True)\n",
    "X_test.replace(mapping, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking data type of categorical columns\n",
    "X_train[[col for col in category_columns1 if col not in ['churn']]].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using PCA as dimensionality reduction technique which is mentioned in Upgrad problem statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = Pipeline([('scaler', StandardScaler()), ('pca', PCA())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit(X_train)\n",
    "churn_pca = pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract pca model from pipeline\n",
    "pca = pca.named_steps['pca']\n",
    "\n",
    "# look at explainded variance of PCA components\n",
    "print(pd.Series(np.round(pca.explained_variance_ratio_.cumsum(), 4)*100)); print_dashes_and_ln();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot feature variance\n",
    "features = range(pca.n_components_)\n",
    "cumulative_variance = np.round(np.cumsum(pca.explained_variance_ratio_)*100, decimals=4)\n",
    "plt.figure(figsize=(175/20,100/20)) # 100 elements on y-axis; 175 elements on x-axis; 20 is normalising factor\n",
    "plt.plot(cumulative_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# As we can see from above graph and table that 60 variables explain 90% variance and 80 variables explain 95% variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA and Logistic Regression\n",
    "# create pipeline\n",
    "PCA_VARS = 60\n",
    "steps = [('scaler', StandardScaler()),\n",
    "         (\"pca\", PCA(n_components=PCA_VARS)),\n",
    "         (\"logistic\", LogisticRegression(class_weight='balanced'))\n",
    "        ]\n",
    "pipeline = Pipeline(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# check score on train data\n",
    "pipeline.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking on Test Data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# create confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm); print_dashes_and_ln();\n",
    "\n",
    "print(\"Accuracy Score ==> {}\".format(round(accuracy_score(y_test,y_pred),2))); print_dashes_and_ln();\n",
    "print(\"AUC Score ==> {}\".format(round(roc_auc_score(y_test,y_pred),2))); print_dashes_and_ln();\n",
    "\n",
    "TP = (confusion_matrix(y_test,y_pred))[0][0]\n",
    "FP = (confusion_matrix(y_test,y_pred))[0][1]\n",
    "FN = (confusion_matrix(y_test,y_pred))[1][0]\n",
    "TN = (confusion_matrix(y_test,y_pred))[1][1]\n",
    "print(\"Not-Churn Accuracy Rate:(Specificity) ==> {}\".format(round(TP/(TP+FP),3))); print_dashes_and_ln();\n",
    "print(\"Churn Accuracy Rate:(Sensitivity) ==> {}\".format(round(TN/(TN+FN),3))); print_dashes_and_ln();\n",
    "\n",
    "# check area under curve\n",
    "y_pred_prob = pipeline.predict_proba(X_test)[:, 1]\n",
    "print(\"AUC Area: \", round(roc_auc_score(y_test, y_pred_prob),3)); print_dashes_and_ln();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning PCA and Logistic Regression\n",
    "# Checing class imbalance\n",
    "y_train.value_counts()/y_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "pca = PCA()\n",
    "\n",
    "# logistic regression - the class weight is used to handle class imbalance - it adjusts the cost function\n",
    "logistic = LogisticRegression(class_weight={0:0.1, 1: 0.9})\n",
    "\n",
    "# create pipeline\n",
    "steps = [(\"scaler\", StandardScaler()), \n",
    "         (\"pca\", pca),\n",
    "         (\"logistic\", logistic)\n",
    "        ]\n",
    "\n",
    "# compile pipeline\n",
    "pca_logistic = Pipeline(steps)\n",
    "\n",
    "# hyperparameter space\n",
    "params = {'pca__n_components': [60, 80], 'logistic__C': [0.1, 0.5, 1, 2, 3, 4, 5, 10], 'logistic__penalty': ['l1', 'l2']}\n",
    "\n",
    "# create 5 folds\n",
    "folds = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 4)\n",
    "\n",
    "# create gridsearch object\n",
    "model = GridSearchCV(estimator=pca_logistic, cv=folds, param_grid=params, scoring='roc_auc', n_jobs=-1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation results\n",
    "pd.DataFrame(model.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print best hyperparameters\n",
    "print(\"Best AUC: \", model.best_score_); print_dashes_and_ln();\n",
    "print(\"Best hyperparameters: \", model.best_params_); print_dashes_and_ln();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict churn on test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# create onfusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm); print_dashes_and_ln();\n",
    "\n",
    "print(\"Accuracy Score ==> {}\".format(round(accuracy_score(y_test,y_pred),2))); print_dashes_and_ln();\n",
    "print(\"AUC Score ==> {}\".format(round(roc_auc_score(y_test,y_pred),2))); print_dashes_and_ln();\n",
    "\n",
    "TP = (confusion_matrix(y_test,y_pred))[0][0]\n",
    "FP = (confusion_matrix(y_test,y_pred))[0][1]\n",
    "FN = (confusion_matrix(y_test,y_pred))[1][0]\n",
    "TN = (confusion_matrix(y_test,y_pred))[1][1]\n",
    "print(\"Not-Churn Accuracy Rate:(Specificity) ==> {}\".format(round(TP/(TP+FP),3))); print_dashes_and_ln();\n",
    "print(\"Churn Accuracy Rate:(Sensitivity) ==> {}\".format(round(TN/(TN+FN),3))); print_dashes_and_ln();\n",
    "\n",
    "# check area under curve\n",
    "y_pred_prob = model.predict_proba(X_test)[:, 1]\n",
    "print(\"AUC Area: \", round(roc_auc_score(y_test, y_pred_prob),3)); print_dashes_and_ln();\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will check this with Random Forest\n",
    "\n",
    "# random forest - the class weight is used to handle class imbalance - it adjusts the cost function\n",
    "forest = RandomForestClassifier(class_weight={0:0.1, 1: 0.9}, n_jobs = -1)\n",
    "\n",
    "# hyperparameter space\n",
    "params = {\"criterion\": ['gini', 'entropy'], \"max_features\": ['auto', 0.4]}\n",
    "\n",
    "# create 5 folds\n",
    "folds = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 4)\n",
    "\n",
    "# create gridsearch object\n",
    "model = GridSearchCV(estimator=forest, cv=folds, param_grid=params, scoring='roc_auc', n_jobs=-1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print best hyperparameters\n",
    "print(\"Best AUC: \", model.best_score_); print_dashes_and_ln();\n",
    "print(\"Best hyperparameters: \", model.best_params_); print_dashes_and_ln();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict churn on test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# create onfusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm); print_dashes_and_ln();\n",
    "\n",
    "\n",
    "print(\"Accuracy Score ==> {}\".format(round(accuracy_score(y_test,y_pred),2))); print_dashes_and_ln();\n",
    "print(\"AUC Score ==> {}\".format(round(roc_auc_score(y_test,y_pred),2))); print_dashes_and_ln();\n",
    "\n",
    "TP = (confusion_matrix(y_test,y_pred))[0][0]\n",
    "FP = (confusion_matrix(y_test,y_pred))[0][1]\n",
    "FN = (confusion_matrix(y_test,y_pred))[1][0]\n",
    "TN = (confusion_matrix(y_test,y_pred))[1][1]\n",
    "print(\"Not-Churn Accuracy Rate:(Specificity) ==> {}\".format(round(TP/(TP+FP),3))); print_dashes_and_ln();\n",
    "print(\"Churn Accuracy Rate:(Sensitivity) ==> {}\".format(round(TN/(TN+FN),3))); print_dashes_and_ln();\n",
    "\n",
    "# check area under curve\n",
    "y_pred_prob = model.predict_proba(X_test)[:, 1]\n",
    "print(\"AUC Area: \", round(roc_auc_score(y_test, y_pred_prob),3)); print_dashes_and_ln();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensitivity for this model is around 46% which is very less. we are going with PCA along with logisitic model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run a random forest model on train data\n",
    "max_features = int(round(np.sqrt(X_train.shape[1])))   \n",
    "# number of variables to consider to split each node\n",
    "print(max_features); print_dashes_and_ln();\n",
    "rf_model = RandomForestClassifier(n_estimators=100, max_features=max_features, class_weight={0:0.1, 1: 0.9}, oob_score=True, random_state=4, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OOB score\n",
    "rf_model.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict churn on test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# create onfusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm); print_dashes_and_ln();\n",
    "\n",
    "\n",
    "print(\"Accuracy Score ==> {}\".format(round(accuracy_score(y_test,y_pred),2))); print_dashes_and_ln();\n",
    "print(\"AUC Score ==> {}\".format(round(roc_auc_score(y_test,y_pred),2))); print_dashes_and_ln();\n",
    "\n",
    "TP = (confusion_matrix(y_test,y_pred))[0][0]\n",
    "FP = (confusion_matrix(y_test,y_pred))[0][1]\n",
    "FN = (confusion_matrix(y_test,y_pred))[1][0]\n",
    "TN = (confusion_matrix(y_test,y_pred))[1][1]\n",
    "print(\"Not-Churn Accuracy Rate:(Specificity) ==> {}\".format(round(TP/(TP+FP),3))); print_dashes_and_ln();\n",
    "print(\"Churn Accuracy Rate:(Sensitivity) ==> {}\".format(round(TN/(TN+FN),3))); print_dashes_and_ln();\n",
    "\n",
    "# check area under curve\n",
    "y_pred_prob = model.predict_proba(X_test)[:, 1]\n",
    "print(\"AUC Area: \", round(roc_auc_score(y_test, y_pred_prob),3)); print_dashes_and_ln();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Inportance\n",
    "# predictors\n",
    "features = rawchurn.drop('churn', axis=1).columns\n",
    "\n",
    "# feature_importance\n",
    "importance = rf_model.feature_importances_\n",
    "\n",
    "# create dataframe\n",
    "feature_importance = pd.DataFrame({'variables': features, 'importance_percentage': importance*100})\n",
    "feature_importance = feature_importance[['variables', 'importance_percentage']]\n",
    "\n",
    "# sort features\n",
    "feature_importance = feature_importance.sort_values('importance_percentage', ascending=False).reset_index(drop=True)\n",
    "print(\"Sum of importance=\", feature_importance.importance_percentage.sum()); print_dashes_and_ln();\n",
    "feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extarcting top 25 Features\n",
    "\n",
    "top_n = 25\n",
    "top_features = feature_importance.variables[0:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heat map\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize = (15,15))      \n",
    "sns.heatmap(data=X_train[top_features].corr(),annot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features = ['total_ic_mou_8', 'total_rech_amt_diff', 'total_og_mou_8', 'arpu_8', 'roam_ic_mou_8', 'roam_og_mou_8', \n",
    "                'std_ic_mou_8', 'std_og_mou_diff']\n",
    "X_train = X_train[top_features]\n",
    "X_test = X_test[top_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "steps = [('scaler', StandardScaler()), \n",
    "         (\"logistic\", LogisticRegression(class_weight={0:0.1, 1:0.9}))\n",
    "        ]\n",
    "\n",
    "# compile pipeline\n",
    "logistic = Pipeline(steps)\n",
    "\n",
    "# hyperparameter space\n",
    "params = {'logistic__C': [0.1, 0.5, 1, 2, 3, 4, 5, 10], 'logistic__penalty': ['l1', 'l2']}\n",
    "\n",
    "# create 5 folds\n",
    "folds = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 4)\n",
    "\n",
    "# create gridsearch object\n",
    "model = GridSearchCV(estimator=logistic, cv=folds, param_grid=params, scoring='roc_auc', n_jobs=-1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print best hyperparameters\n",
    "print(\"Best AUC: \", model.best_score_); print_dashes_and_ln();\n",
    "print(\"Best hyperparameters: \", model.best_params_); print_dashes_and_ln();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict churn on test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# create onfusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm); print_dashes_and_ln();\n",
    "\n",
    "\n",
    "print(\"Accuracy Score ==> {}\".format(round(accuracy_score(y_test,y_pred),2))); print_dashes_and_ln();\n",
    "print(\"AUC Score ==> {}\".format(round(roc_auc_score(y_test,y_pred),2))); print_dashes_and_ln();\n",
    "\n",
    "TP = (confusion_matrix(y_test,y_pred))[0][0]\n",
    "FP = (confusion_matrix(y_test,y_pred))[0][1]\n",
    "FN = (confusion_matrix(y_test,y_pred))[1][0]\n",
    "TN = (confusion_matrix(y_test,y_pred))[1][1]\n",
    "print(\"Not-Churn Accuracy Rate:(Specificity) ==> {}\".format(round(TP/(TP+FP),3))); print_dashes_and_ln();\n",
    "print(\"Churn Accuracy Rate:(Sensitivity) ==> {}\".format(round(TN/(TN+FN),3))); print_dashes_and_ln();\n",
    "\n",
    "# check area under curve\n",
    "y_pred_prob = model.predict_proba(X_test)[:, 1]\n",
    "print(\"AUC Area: \", round(roc_auc_score(y_test, y_pred_prob),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model = model.best_estimator_.named_steps['logistic']\n",
    "# intercept\n",
    "intercept_df = pd.DataFrame(logistic_model.intercept_.reshape((1,1)), columns = ['intercept'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coefficients\n",
    "coefficients = logistic_model.coef_.reshape((8, 1)).tolist()\n",
    "coefficients = [val for sublist in coefficients for val in sublist]\n",
    "coefficients = [round(coefficient, 3) for coefficient in coefficients]\n",
    "\n",
    "logistic_features = list(X_train.columns)\n",
    "coefficients_df = pd.DataFrame(logistic_model.coef_, columns=logistic_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate dataframes\n",
    "coefficients = pd.concat([intercept_df, coefficients_df], axis=1)\n",
    "coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Minutes of Usage on 8th Month for outgoing and incoming calls (Mostly Roaming/Local/STD) and \n",
    "2. Recharge amount difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation\n",
    "1. If the total usage as measured by the total minutes of usage and the recharge amount in 7th and 8th month is declining as compared to 6th month, then it is likely that such a customer will churn. \n",
    "2. If the Total Outgoing Minutes of Usage falls below 220 minutes in the 8th, We recommend the telecom provider to reach out to such customers and provide them with lockin offers that will prevent their churn.\n",
    "3. Telecom provider should also focus on STD and roaming rates"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
